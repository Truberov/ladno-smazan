# ladno-smazan

![kandinsky-download-1727581197737](assets/123.png)

 # :robot: AI Assistant for Advertising Agency Data Search 

### Команда AiRina представляет решение по разработке интеллектуального помощника для рекламных агентств
[Ссылка на решение](https://t.me/airina_rutube_bot)


## :exploding_head: Проблематика

Рекламные агентства ежедневно сталкиваются с необходимостью поиска информации в обширных массивах данных, включая текстовые документы и презентационные материалы. Используемые системы управления данными не всегда обеспечивают удобный и быстрый доступ к нужной информации, что замедляет процесс принятия решений и разработки стратегий. Кроме того, постоянное обновление данных требует гибкости и оперативности, чтобы поддерживать конкурентоспособность.

## :hugs: Решение

Решением данной задачи станет чат-бот с веб-интерфейсом, который сможет быстро находить и предлагать релевантную информацию из агентской базы данных. Интеллектуальный помощник будет не только распознавать текст и картинки документов, но и предоставляет возможность дополнять библиотеку новыми файлами. Это значительно сократит время на поиск нужных данных и повысит эффективность работы агентства, помогая команде сосредоточиться на анализе и разработке стратегий.


## :building_construction: Архитектра решения

Интеллектуальный чат-бот с веб-интерфейсом построен на RAG-pipeline, который включает в себя:
- Мультимодальный поиск: Colpali
- Инференс: V-LLM
- Генерацию ответа: Vikhr-2.5-VL-2b-Instruct

### :pencil2: Ввод пользователя и oбработка запроса

Происходит нормализация и удаление стоп слов для уменьшение шума на эмбеддингах

> [!Note] 
> Мы посчитали наиболее часто встречающиеся слова на реальных данных (например здравствуйте, спасибо и т.д.)
> и включили их в стоп слова, что положительное повлияло на метрику нахождения релевантых ответов из БЗ

### :mag_right: Поиск запроса по ответам БЗ

Векторным поиском находим топ 30 релеваных документов 

> [!Note]
> Для наибольшей релевантности выполняем поиск не только по собственным векторам вопросам из БЗ, а так же по перефразированным вопросам
>

### :bookmark_tabs: Фильтруем документы

LLM выполняет фильтрацию документов и выбирает k документов для генерации ответа

>[!Note]
>Для наибольшей релевантности выполняем поиск не только по собственным веторам ответов из БЗ, а так же по перефразированным ответа
>

### :bulb: Генерация ответа

На основании отфильтрованных документов, генерируем ответ
> [!Note]
> Но если с фильтрации пришло 0 документов, модель сразу ответит "Я не знаю". Поэтому у модели нет возможности
> придумывать свои ответ. Ответы всегда будут основываться на БЗ
>

### :bricks: Композиция нагядно
![image_2024-09-29_09-33-29](https://github.com/user-attachments/assets/fffde057-0426-4375-b064-49e51a0ffde0)

 # :rocket: Запуск
Решение упаковано и будет готов к работе через **2 строки**

 **Для запуска нужны**
 - docker
 - docker-compose
 - make

**Запуск инференса LLM**
```
vllm serve --dtype half --max-model-len 16000 -tp 1 Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24 --api-key token-abc123
```
   
**Развертывание**
```
make up
```

 # :computer: Стек технологий
**БД**
- redis (хранилище связей аугментированных вопросов и вопрос из бз)
- chroma (векторное хранилище)

**Код**
- python
- langchain

# :checkered_flag: Итог
**В конечном итоге мы предлагалем ready-to-start решение у которого**

:heavy_check_mark: Документы топ-5 95%

:heavy_check_mark: Страницы топ-5 89%

:heavy_check_mark: Accuracy 85% и полная интеграция web-интерфейса

:heavy_check_mark: Высокая степень защиты от галлюцинаций

:heavy_check_mark: 1000 подключений(проверенное количество)


## made with ♥️ by AiRina for 
![header-logo c7e8f395](https://github.com/user-attachments/assets/8a56ca15-e17a-4ab6-b864-017fce804610)



