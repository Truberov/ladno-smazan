# ladno-smazan

![kandinsky-download-1727581197737](assets/123.png)

 # :robot: AI Assistant for Advertising Agency Data Search 

### Команда AiRina представляет решение по разработке интеллектуального помощника для рекламных агентств
[Ссылка на решение](https://t.me/airina_rutube_bot)


## :exploding_head: Проблематика

Рекламные агентства ежедневно сталкиваются с необходимостью поиска информации в обширных массивах данных, включая текстовые документы и презентационные материалы. Используемые системы управления данными не всегда обеспечивают удобный и быстрый доступ к нужной информации, что замедляет процесс принятия решений и разработки стратегий. Кроме того, постоянное обновление данных требует гибкости и оперативности, чтобы поддерживать конкурентоспособность.

## :hugs: Решение

Решением данной задачи станет чат-бот с веб-интерфейсом, который сможет быстро находить и предлагать релевантную информацию из агентской базы данных. Интеллектуальный помощник будет не только распознавать текст и картинки документов, но и предоставляет возможность дополнять библиотеку новыми файлами. Это значительно сократит время на поиск нужных данных и повысит эффективность работы агентства, помогая команде сосредоточиться на анализе и разработке стратегий.


## :building_construction: Архитектра решения

Интеллектуальный чат-бот с веб-интерфейсом построен на RAG-pipeline, который включает в себя:
- Мультимодальный поиск: Colpali
- Инференс: V-LLM
- Генерацию ответа: Vikhr-2.5-VL-2b-Instruct

### :pencil2: Ввод пользователя и oбработка запроса

Происходит обработка данных

> [!Note] 
> Удалили фон документов,что увеличило точность поиска нужной информации на 5-10%.

### :mag_right: Поиск запроса по ответам БЗ

Добавили ранжирование в поиск 

> [!Note]
> Для наибольшей релевантности добавили расширенный датасет реальных ответов,что увеличило точность поиска на 5%.

### :bookmark_tabs: Квантизация и дообучении LLM

LLM  не требует больших вычислительных ресурсов,что позволило нам увеличить скорость ответа и сэкономить ресурсы.


### :bulb: Дополнительная проверка на фантазии 

На основании целевых вопросов пользователей, генерируем ответ
> [!Note]
> Но если с фильтрации пришло 0 документов, модель сразу ответит "Я не знаю". Поэтому у модели нет возможности придумывать свои ответ. Ответы всегда будут основываться на БЗ
>

### :bricks: Композиция нагядно
![image_2024-09-29_09-33-29](https://github.com/user-attachments/assets/fffde057-0426-4375-b064-49e51a0ffde0)

 # :rocket: Запуск
Решение упаковано и будет готов к работе через **2 строки**

 **Для запуска нужны**
 - docker
 - docker-compose
 - make

**Запуск инференса LLM**
```
vllm serve --dtype half --max-model-len 16000 -tp 1 Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24 --api-key token-abc123
```
   
**Развертывание**
```
make up
```

 # :computer: Стек технологий
**БД**
- redis (хранилище связей аугментированных вопросов и вопрос из бз)
- chroma (векторное хранилище)

**Код**
- python
- langchain

# :checkered_flag: Итог
**В конечном итоге мы предлагалем ready-to-start решение у которого**

:heavy_check_mark: Документы топ-5 95%

:heavy_check_mark: Страницы топ-5 89%

:heavy_check_mark: Accuracy 85% и полная интеграция web-интерфейса

:heavy_check_mark: Высокая степень защиты от галлюцинаций

:heavy_check_mark: 1000 подключений(проверенное количество)


## made with ♥️ by AiRina for 
![header-logo c7e8f395](assets/12345.png)



