# ladno-smazan

![kandinsky-download-1727581197737](assets/123.png)

 # :robot: AI Assistant for Advertising Agency Data Search 

### Команда AiRina представляет решение по разработке интеллектуального помощника для рекламных агентств


## :building_construction: ## Описание проекта

Этот проект представляет собой готовое решение для рекламных агентств, позволяющее эффективно искать информацию в больших массивах данных, что ускоряет процесс принятия решений и разработки стратегий. Разработанный чат-бот с веб-интерфейсом предоставляет удобные инструменты для быстрого поиска релевантной информации в библиотеке материалов агентства, которая включает текстовую и презентационную информацию.

Чат-бот оперативно находит нужные данные и предоставляет ссылки на искомую информацию, а также позволяет пользователям легко дополнять базу данных новыми файлами. В системе реализованы метрики и лидерборд, которые помогают отслеживать активность пользователей и стимулируют более продуктивное взаимодействие с ботом, способствуя повышению эффективности работы агентства.

Решение реализовано с использованием фреймворка **LangChain** и схема графа выглядит следующим образом:
![Граф](./assets/123456.png) 


## Подготовка данных и создание векторного хранилища

1. **Сбор документов**:
   Пользователь загружает документ,документ разбивается на картинки png.Пользователь может загрузить документы в базу данных более чем в 10 форматах('*.rtf', '*.doc', '*.docx', '*.pptx', '*.csv', '*.txt', '*.pdf', '*.png', '*.jpg').

2. **Создание картинок из pdf файла**:
   Мы используем мультимодальную эмбединг модель для разбиение pdf файлов и других документов.

   Этот шаг разделит pdf файл на страницы и каждую из них преобразует в текст, описав изображения при помощи мультимодальной эмбединг модели. Итоговые тексты сохранятся.

3. **Создание векторного хранилища**:
   После этого,мы создаем векторную базу и храним эмбеддинги документов в chroma db.

## Запуск приложения

Решение упаковано и будет готов к работе через **2 строки**

 **Для запуска нужны**
 - docker
 - docker-compose
 - make

   Теперь вы сможете взаимодействовать с чат-ботом и задавать вопросы.

 # :computer: ## Пример использования

После запуска приложения вы сможете задать свой вопрос, и система сгенерирует ответ на основе информации, найденной в документах,а также выведет результаты похожих слайдов презентаций и журналов в базе.

1. Чат-бот верно отвечает на вопросы по pdf документам
   
   ![Первый пример](./assets/image3.png)

2. Чат-бот получает ответ и от похожих слайдов в базе данных.
   ![Второй пример](./assets/image2.png)

3. Чат-бот позволяет увеличить слайд,который он вывел пользователю для удобства и читаемости.
   ![Третий пример](./assets/image.png)


# :checkered_flag: ## Основные проблемы и решения

## :exploding_head: ### Обработка исходных PDF документов

Мультимодальный ретривер ColPali (https://huggingface.co/vidore/colpali-v1.3)
Основная сложность заключалась в том, что важные сведения были распределены между визуальными и текстовыми частями документа. К примеру, количественные показатели могли находиться в одном изображении, а их объяснения - либо в другом изображении, либо в тексте. Из-за такой фрагментированной организации информации было непросто корректно соединить и осмыслить все связанные данные. Изначально использовали библиотеку docling для обработки таких документов c пременением OCR: EasyOCR, Tesseract, Surya но это обработка затянулась на часы.
 

## :hugs: ### Принятое решение

мультимодальный ретривер, захватывающий текст и изображение в единый вектор.
Выбрали ColPali потому что:
а) в 20 раз быстрее библиотек docling+OCR (сейчас страница обрабатывается около 20сек)
б) является одним из лучших мультимодальных ретриверов. ссылка(https://huggingface.co/spaces/vidore/vidore-leaderboard)
в) не взяли топ-1 colbert потому что на нашем датасете (500 вопросов, 50/50 ручные и синтетические). Очевидно потому что colpali обучался на multilingual-датасете 

2. LLM Vikhr-2.5-VL-2b-Instruct, потому что:
а) Обучена на российском наборе данных
б) основана на QWEN c открытой лицензией до 100млн пользователей 

3. Инференс: V-LLM, потому что:
а) Быстрая генерация параллельных ответов
б) Легко масштабируется

Инференс: GPU 1x4080 16Gb


## made with ♥️ by AiRina for 
![header-logo c7e8f395](assets/12345.png)



